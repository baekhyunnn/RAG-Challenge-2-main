import os
import json
import faiss
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from dotenv import load_dotenv
import dashscope
from dashscope import TextEmbedding
from pathlib import Path
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# åŠ è½½ç¯å¢ƒå˜é‡å¹¶åˆå§‹åŒ–DashScope
load_dotenv()
dashscope.api_key = os.getenv("DASHSCOPE_API_KEY")
if not dashscope.api_key:
    raise EnvironmentError("âŒ æœªæ‰¾åˆ°DASHSCOPE_API_KEYï¼Œè¯·é…ç½®.envæ–‡ä»¶")

# å…¨å±€é…ç½®
CONFIG = {
    "EMBEDDING_MODEL": TextEmbedding.Models.text_embedding_v1,
    "VECTOR_STORE_DIR": Path("vector_store"),  # ä¸ingestion.pyå‘é‡åº“ç›®å½•ä¸€è‡´
    "TOP_K_DEFAULT": 5,  # é»˜è®¤æ£€ç´¢æ•°é‡
    "SIMILARITY_THRESHOLD": 0.7,  # ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä½äºæ­¤å€¼çš„ç»“æœä¼šè¢«è¿‡æ»¤
    "MAX_CHUNK_LENGTH": 1000,  # æœ€å¤§æ–‡æœ¬å—é•¿åº¦ï¼ˆå­—ç¬¦ï¼‰
    "ENABLE_RERANKING": True,  # æ˜¯å¦å¯ç”¨é‡æ’åº
}

class VectorStoreManager:
    """å‘é‡åº“ç®¡ç†å™¨ï¼Œè´Ÿè´£åŠ è½½å’Œç®¡ç†å‘é‡ç´¢å¼•"""
    
    def __init__(self):
        self.vector_store_dir = CONFIG["VECTOR_STORE_DIR"]
        self.merged_index_path = self.vector_store_dir / "merged_faiss.index"
        self.merged_chunks_path = self.vector_store_dir / "merged_chunks.json"
        self.indices = {}  # ç¼“å­˜åŠ è½½çš„ç´¢å¼•
        self.chunks_data = {}  # ç¼“å­˜æ–‡æœ¬å—æ•°æ®
        
    def load_merged_index(self) -> Tuple[Optional[faiss.Index], Optional[Dict]]:
        """
        åŠ è½½åˆå¹¶çš„å‘é‡ç´¢å¼•
        :return: (ç´¢å¼•å¯¹è±¡, æ–‡æœ¬å—æ•°æ®) æˆ– (None, None)
        """
        try:
            if not self.merged_index_path.exists() or not self.merged_chunks_path.exists():
                logger.warning("âš ï¸  æœªæ‰¾åˆ°åˆå¹¶çš„å‘é‡ç´¢å¼•ï¼Œå°è¯•åŠ è½½å•ä¸ªæ–‡æ¡£ç´¢å¼•")
                return None, None
            
            # åŠ è½½åˆå¹¶ç´¢å¼•
            index = faiss.read_index(str(self.merged_index_path))
            
            # åŠ è½½åˆå¹¶çš„æ–‡æœ¬å—æ•°æ®
            with open(self.merged_chunks_path, "r", encoding="utf-8") as f:
                chunks_data = json.load(f)
            
            logger.info(f"âœ… å·²åŠ è½½åˆå¹¶å‘é‡ç´¢å¼•ï¼ŒåŒ…å« {chunks_data.get('total_chunks', 0)} ä¸ªæ–‡æœ¬å—")
            return index, chunks_data
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½åˆå¹¶ç´¢å¼•å¤±è´¥ï¼š{str(e)}")
            return None, None
    
    def load_single_indices(self) -> List[Tuple[str, faiss.Index, Dict]]:
        """
        åŠ è½½æ‰€æœ‰å•ä¸ªæ–‡æ¡£çš„å‘é‡ç´¢å¼•
        :return: åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º(æ–‡æ¡£å, ç´¢å¼•å¯¹è±¡, æ–‡æœ¬å—æ•°æ®)
        """
        indices_list = []
        
        # æŸ¥æ‰¾æ‰€æœ‰ç´¢å¼•æ–‡ä»¶
        index_files = list(self.vector_store_dir.glob("*_faiss.index"))
        
        for index_file in index_files:
            try:
                # è§£ææ–‡æ¡£ä¿¡æ¯
                file_name = index_file.stem
                if "_faiss" in file_name:
                    doc_name = file_name.replace("_faiss", "")
                    doc_type = "pdf" if "pdf" in file_name.lower() else "doc"
                else:
                    doc_name = file_name
                    doc_type = "unknown"
                
                # æŸ¥æ‰¾å¯¹åº”çš„æ–‡æœ¬å—æ–‡ä»¶
                chunks_pattern = f"*{doc_name}*chunks.json"
                chunks_files = list(self.vector_store_dir.glob(chunks_pattern))
                
                if not chunks_files:
                    logger.warning(f"âš ï¸  è·³è¿‡ {doc_name}ï¼šæ— å¯¹åº”æ–‡æœ¬å—æ–‡ä»¶")
                    continue
                
                chunks_file = chunks_files[0]
                
                # åŠ è½½ç´¢å¼•
                index = faiss.read_index(str(index_file))
                
                # åŠ è½½æ–‡æœ¬å—æ•°æ®
                with open(chunks_file, "r", encoding="utf-8") as f:
                    chunks_data = json.load(f)
                
                indices_list.append((doc_name, doc_type, index, chunks_data))
                logger.debug(f"âœ… å·²åŠ è½½ {doc_name} ({doc_type}) çš„å‘é‡ç´¢å¼•")
                
            except Exception as e:
                logger.error(f"âŒ åŠ è½½ {index_file.name} å¤±è´¥ï¼š{str(e)}")
                continue
        
        logger.info(f"âœ… å·²åŠ è½½ {len(indices_list)} ä¸ªæ–‡æ¡£çš„å‘é‡ç´¢å¼•")
        return indices_list
    
    def get_all_indices(self):
        """
        è·å–æ‰€æœ‰å¯ç”¨çš„å‘é‡ç´¢å¼•
        :return: ä¼˜å…ˆè¿”å›åˆå¹¶ç´¢å¼•ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›æ‰€æœ‰å•ä¸ªç´¢å¼•
        """
        # å…ˆå°è¯•åŠ è½½åˆå¹¶ç´¢å¼•
        merged_index, merged_chunks = self.load_merged_index()
        if merged_index and merged_chunks:
            return [("merged", "merged", merged_index, merged_chunks)]
        
        # å¦‚æœæ²¡æœ‰åˆå¹¶ç´¢å¼•ï¼ŒåŠ è½½æ‰€æœ‰å•ä¸ªç´¢å¼•
        return self.load_single_indices()

def get_query_embedding(query: str) -> np.ndarray:
    """
    ç”ŸæˆæŸ¥è¯¢è¯­å¥çš„Embeddingï¼ˆè°ƒç”¨DashScopeï¼‰
    :param query: ç”¨æˆ·æŸ¥è¯¢é—®é¢˜
    :return: å½’ä¸€åŒ–åçš„æŸ¥è¯¢å‘é‡
    """
    if not query or query.strip() == "":
        raise ValueError("æŸ¥è¯¢è¯­å¥ä¸èƒ½ä¸ºç©º")
    
    try:
        # è°ƒç”¨DashScopeç”Ÿæˆå•å¥Embedding
        resp = TextEmbedding.call(
            model=CONFIG["EMBEDDING_MODEL"],
            input=[query.strip()]
        )
        
        if resp.status_code != 200:
            logger.error(f"âŒ Embeddingç”Ÿæˆå¤±è´¥ï¼š{resp.message}")
            raise ValueError(f"Embeddingç”Ÿæˆå¤±è´¥ï¼š{resp.message}")
        
        # æå–å¹¶è½¬æ¢å‘é‡
        emb = np.array(resp["output"]["embeddings"][0]["embedding"], dtype=np.float32)
        
        # å½’ä¸€åŒ–ï¼ˆä¸FAISSå†…ç§¯ç´¢å¼•åŒ¹é…ï¼‰
        emb = emb.reshape(1, -1)
        faiss.normalize_L2(emb)
        
        logger.debug(f"âœ… æŸ¥è¯¢Embeddingç”ŸæˆæˆåŠŸï¼Œç»´åº¦ï¼š{emb.shape}")
        return emb
        
    except Exception as e:
        logger.error(f"âŒ ç”ŸæˆæŸ¥è¯¢Embeddingæ—¶å‡ºé”™ï¼š{str(e)}")
        raise

def rerank_results(query: str, results: List[Dict], top_k: int = 5) -> List[Dict]:
    """
    å¯¹æ£€ç´¢ç»“æœè¿›è¡Œé‡æ’åº
    :param query: æŸ¥è¯¢é—®é¢˜
    :param results: æ£€ç´¢ç»“æœåˆ—è¡¨
    :param top_k: è¿”å›æ•°é‡
    :return: é‡æ’åºåçš„ç»“æœ
    """
    if not CONFIG["ENABLE_RERANKING"] or len(results) <= 1:
        return results[:top_k]
    
    try:
        # ç®€å•çš„åŸºäºå…³é”®è¯å’Œé•¿åº¦çš„é‡æ’åºç­–ç•¥
        # å¯ä»¥æ ¹æ®éœ€æ±‚æ›¿æ¢ä¸ºæ›´å¤æ‚çš„é‡æ’åºæ¨¡å‹
        
        def calculate_score(chunk: str, query: str) -> float:
            """è®¡ç®—æ–‡æœ¬å—çš„ç›¸å…³æ€§å¾—åˆ†"""
            chunk_lower = chunk.lower()
            query_lower = query.lower()
            
            # 1. å…³é”®è¯åŒ¹é…å¾—åˆ†
            keyword_score = 0
            query_words = set(query_lower.split())
            chunk_words = set(chunk_lower.split())
            
            matched_words = query_words.intersection(chunk_words)
            if matched_words:
                keyword_score = len(matched_words) / len(query_words)
            
            # 2. é•¿åº¦å¾—åˆ†ï¼ˆä¼˜å…ˆé€‚ä¸­é•¿åº¦çš„æ–‡æœ¬å—ï¼‰
            chunk_len = len(chunk)
            if chunk_len < 100:  # å¤ªçŸ­çš„æ–‡æœ¬å—å¾—åˆ†é™ä½
                length_score = 0.5
            elif chunk_len > 800:  # å¤ªé•¿çš„æ–‡æœ¬å—å¾—åˆ†é™ä½
                length_score = 0.7
            else:
                length_score = 1.0
            
            # 3. ç»“æ„å¾—åˆ†ï¼ˆåŒ…å«é—®é¢˜/ç­”æ¡ˆæ ¼å¼çš„å¾—åˆ†æ›´é«˜ï¼‰
            structure_score = 1.0
            question_markers = ["é—®é¢˜ï¼š", "qï¼š", "é—®ï¼š", "é¢˜ç›®ï¼š", "è¯•é¢˜ï¼š", "ï¼Ÿ"]
            answer_markers = ["ç­”æ¡ˆï¼š", "aï¼š", "ç­”ï¼š", "è§£ç­”ï¼š", "è§£æï¼š"]
            
            has_question = any(marker in chunk_lower for marker in question_markers)
            has_answer = any(marker in chunk_lower for marker in answer_markers)
            
            if has_question and has_answer:
                structure_score = 1.5  # åŒæ—¶åŒ…å«é—®é¢˜å’Œç­”æ¡ˆçš„æ–‡æœ¬å—å¾—åˆ†æ›´é«˜
            elif has_question or has_answer:
                structure_score = 1.2  # åŒ…å«å…¶ä¸­ä¸€ä¸ªçš„å¾—åˆ†ç¨é«˜
            
            # ç»¼åˆå¾—åˆ†
            total_score = (keyword_score * 0.4 + 
                          length_score * 0.2 + 
                          structure_score * 0.4)
            
            return total_score
        
        # ä¸ºæ¯ä¸ªç»“æœè®¡ç®—å¾—åˆ†
        scored_results = []
        for result in results:
            score = calculate_score(result["chunk"], query)
            scored_results.append({
                **result,
                "rerank_score": score
            })
        
        # æŒ‰å¾—åˆ†æ’åº
        scored_results.sort(key=lambda x: x["rerank_score"], reverse=True)
        
        logger.info(f"âœ… é‡æ’åºå®Œæˆï¼Œè¿”å›å‰ {top_k} ä¸ªç»“æœ")
        return scored_results[:top_k]
        
    except Exception as e:
        logger.warning(f"âš ï¸  é‡æ’åºå¤±è´¥ï¼Œè¿”å›åŸå§‹ç»“æœï¼š{str(e)}")
        return results[:top_k]

def retrieve_similar_chunks(
    query: str, 
    top_k: int = None, 
    doc_filter: Optional[str] = None,
    similarity_threshold: float = None
) -> List[Dict[str, Any]]:
    """
    æ£€ç´¢ä¸æŸ¥è¯¢æœ€ç›¸ä¼¼çš„æ–‡æœ¬å—ï¼ˆå¢å¼ºç‰ˆï¼‰
    :param query: ç”¨æˆ·æŸ¥è¯¢é—®é¢˜
    :param top_k: è¿”å›æœ€ç›¸ä¼¼çš„kä¸ªæ–‡æœ¬å—
    :param doc_filter: æ–‡æ¡£è¿‡æ»¤å™¨ï¼ˆæŒ‡å®šæ£€ç´¢æŸä¸ªæ–‡æ¡£ï¼‰
    :param similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
    :return: æ£€ç´¢ç»“æœåˆ—è¡¨ï¼ŒåŒ…å«è¯¦ç»†å…ƒæ•°æ®
    """
    if top_k is None:
        top_k = CONFIG["TOP_K_DEFAULT"]
    
    if similarity_threshold is None:
        similarity_threshold = CONFIG["SIMILARITY_THRESHOLD"]
    
    try:
        # 1. ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_emb = get_query_embedding(query)
        logger.info(f"ğŸ” å¼€å§‹æ£€ç´¢ï¼š'{query}'")
        
        # 2. åˆå§‹åŒ–å‘é‡åº“ç®¡ç†å™¨
        store_manager = VectorStoreManager()
        
        # 3. è·å–æ‰€æœ‰å‘é‡ç´¢å¼•
        all_indices = store_manager.get_all_indices()
        if not all_indices:
            logger.error("âŒ æ— å¯ç”¨å‘é‡ç´¢å¼•ï¼Œè¯·å…ˆè¿è¡Œ ingestion.py")
            return []
        
        # 4. æ‰§è¡Œæ£€ç´¢
        all_results = []
        
        for doc_name, doc_type, index, chunks_data in all_indices:
            # åº”ç”¨æ–‡æ¡£è¿‡æ»¤å™¨
            if doc_filter and doc_name != doc_filter and doc_filter != "all":
                continue
            
            # è·å–æ–‡æœ¬å—åˆ—è¡¨
            text_chunks = chunks_data.get("chunks", [])
            if not text_chunks:
                logger.warning(f"âš ï¸  {doc_name} æ— æ–‡æœ¬å—æ•°æ®")
                continue
            
            # è®¾ç½®æ£€ç´¢æ•°é‡ï¼ˆé’ˆå¯¹å•ä¸ªç´¢å¼•ï¼‰
            search_k = min(top_k * 2, len(text_chunks))  # æ£€ç´¢ç¨å¤šä¸€äº›çš„ç»“æœç”¨äºåç»­ç­›é€‰
            
            # æ‰§è¡Œç›¸ä¼¼åº¦æ£€ç´¢
            distances, indices = index.search(query_emb, search_k)
            
            # å¤„ç†æ£€ç´¢ç»“æœ
            for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
                if idx < 0 or idx >= len(text_chunks):  # æ— æ•ˆç´¢å¼•
                    continue
                
                # åº”ç”¨ç›¸ä¼¼åº¦é˜ˆå€¼
                if distance < similarity_threshold:
                    continue
                
                chunk_text = text_chunks[idx]
                
                # æˆªæ–­è¿‡é•¿çš„æ–‡æœ¬å—
                if len(chunk_text) > CONFIG["MAX_CHUNK_LENGTH"]:
                    chunk_text = chunk_text[:CONFIG["MAX_CHUNK_LENGTH"]] + "..."
                
                result = {
                    "chunk": chunk_text,
                    "similarity": float(distance),
                    "doc_name": doc_name,
                    "doc_type": doc_type,
                    "chunk_index": int(idx),
                    "total_chunks_in_doc": len(text_chunks),
                    "source_info": f"æ¥è‡ªæ–‡æ¡£ï¼š{doc_name} ({doc_type})"
                }
                
                all_results.append(result)
        
        if not all_results:
            logger.warning("âš ï¸  æœªæ£€ç´¢åˆ°ç›¸ä¼¼æ–‡æœ¬å—ï¼Œå°è¯•é™ä½ç›¸ä¼¼åº¦é˜ˆå€¼")
            # å¦‚æœæ²¡æ‰¾åˆ°ç»“æœï¼Œæ”¾å®½é˜ˆå€¼é‡æ–°æ£€ç´¢
            if similarity_threshold > 0.3:
                return retrieve_similar_chunks(
                    query, top_k, doc_filter, similarity_threshold - 0.1
                )
            return []
        
        # 5. æŒ‰ç›¸ä¼¼åº¦æ’åº
        all_results.sort(key=lambda x: x["similarity"], reverse=True)
        
        # 6. å»é‡ï¼ˆåŸºäºæ–‡æœ¬å†…å®¹çš„å»é‡ï¼‰
        unique_results = []
        seen_chunks = set()
        
        for result in all_results:
            chunk_hash = hash(result["chunk"][:200])  # å–å‰200å­—ç¬¦çš„å“ˆå¸Œä½œä¸ºå»é‡ä¾æ®
            if chunk_hash not in seen_chunks:
                seen_chunks.add(chunk_hash)
                unique_results.append(result)
        
        # 7. é‡æ’åº
        final_results = rerank_results(query, unique_results, top_k)
        
        logger.info(f"âœ… æ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(final_results)} ä¸ªç›¸å…³æ–‡æœ¬å—")
        for i, result in enumerate(final_results[:3], 1):
            logger.debug(f"  ç»“æœ {i}: {result['doc_name']} (ç›¸ä¼¼åº¦: {result['similarity']:.3f})")
        
        return final_results
        
    except Exception as e:
        logger.error(f"âŒ æ£€ç´¢è¿‡ç¨‹ä¸­å‡ºé”™ï¼š{str(e)}")
        return []

def retrieve_chunks_with_context(
    query: str, 
    top_k: int = 5,
    include_context: bool = True
) -> List[Dict[str, Any]]:
    """
    æ£€ç´¢ç›¸ä¼¼æ–‡æœ¬å—ï¼Œå¹¶åŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯
    :param query: ç”¨æˆ·æŸ¥è¯¢
    :param top_k: è¿”å›æ•°é‡
    :param include_context: æ˜¯å¦åŒ…å«ä¸Šä¸‹æ–‡
    :return: åŒ…å«ä¸Šä¸‹æ–‡çš„ç»“æœåˆ—è¡¨
    """
    results = retrieve_similar_chunks(query, top_k=top_k)
    
    if not include_context or not results:
        return results
    
    # åŠ è½½æ‰€æœ‰æ–‡æœ¬å—æ•°æ®ä»¥è·å–ä¸Šä¸‹æ–‡
    store_manager = VectorStoreManager()
    all_indices = store_manager.load_single_indices()
    
    doc_chunks_map = {}
    for doc_name, _, _, chunks_data in all_indices:
        doc_chunks_map[doc_name] = chunks_data.get("chunks", [])
    
    # ä¸ºæ¯ä¸ªç»“æœæ·»åŠ ä¸Šä¸‹æ–‡
    for result in results:
        doc_name = result["doc_name"]
        chunk_idx = result["chunk_index"]
        chunks = doc_chunks_map.get(doc_name, [])
        
        if not chunks:
            continue
        
        # æ·»åŠ ä¸Šä¸‹æ–‡ï¼ˆå‰åå„1ä¸ªchunkï¼‰
        start_idx = max(0, chunk_idx - 1)
        end_idx = min(len(chunks), chunk_idx + 2)  # +2å› ä¸ºåˆ‡ç‰‡æ˜¯å‰é—­åå¼€
        
        context_chunks = chunks[start_idx:end_idx]
        context_text = "\n\n...\n\n".join(context_chunks)
        
        result["context"] = context_text
        result["context_range"] = f"{start_idx+1}-{end_idx}"
    
    return results

def get_available_documents() -> List[Dict[str, str]]:
    """
    è·å–æ‰€æœ‰å¯ç”¨çš„æ–‡æ¡£åˆ—è¡¨
    :return: æ–‡æ¡£ä¿¡æ¯åˆ—è¡¨
    """
    try:
        store_manager = VectorStoreManager()
        indices = store_manager.load_single_indices()
        
        documents = []
        for doc_name, doc_type, _, chunks_data in indices:
            documents.append({
                "name": doc_name,
                "type": doc_type.upper(),
                "chunks_count": chunks_data.get("total_chunks", 0),
                "title": chunks_data.get("doc_name", doc_name)
            })
        
        # æŒ‰æ–‡æ¡£ç±»å‹æ’åº
        documents.sort(key=lambda x: x["type"])
        return documents
        
    except Exception as e:
        logger.error(f"âŒ è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥ï¼š{str(e)}")
        return []

# æµ‹è¯•æ£€ç´¢åŠŸèƒ½
if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ” RAGæ£€ç´¢æ¨¡å—æµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯•1ï¼šè·å–å¯ç”¨æ–‡æ¡£
    print("\nğŸ“‹ å¯ç”¨æ–‡æ¡£åˆ—è¡¨ï¼š")
    docs = get_available_documents()
    for doc in docs:
        print(f"  - {doc['name']} ({doc['type']}): {doc['chunks_count']} ä¸ªæ–‡æœ¬å—")
    
    # æµ‹è¯•2ï¼šåŸºæœ¬æ£€ç´¢
    print("\nğŸ§ª æµ‹è¯•åŸºæœ¬æ£€ç´¢ï¼š")
    test_queries = [
        "Transformeræ¨¡å‹çš„æ ¸å¿ƒæœºåˆ¶æ˜¯ä»€ä¹ˆï¼Ÿ",
        "å¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒéœ€è¦å“ªäº›æ•°æ®ï¼Ÿ",
        "RAGç³»ç»Ÿçš„å·¥ä½œæµç¨‹æ˜¯æ€æ ·çš„ï¼Ÿ"
    ]
    
    for query in test_queries[:1]:  # åªæµ‹è¯•ç¬¬ä¸€ä¸ª
        print(f"\næŸ¥è¯¢ï¼š'{query}'")
        results = retrieve_similar_chunks(query, top_k=3)
        
        if results:
            print(f"æ‰¾åˆ° {len(results)} ä¸ªç»“æœï¼š")
            for i, result in enumerate(results, 1):
                print(f"\n{i}. [{result['doc_name']}] ç›¸ä¼¼åº¦: {result['similarity']:.3f}")
                print(f"   æ¥æºï¼š{result['source_info']}")
                print(f"   å†…å®¹ï¼š{result['chunk'][:150]}...")
        else:
            print("âš ï¸  æœªæ‰¾åˆ°ç›¸å…³ç»“æœ")
    
    print("\n" + "=" * 60)
    print("âœ… æ£€ç´¢æ¨¡å—æµ‹è¯•å®Œæˆ")
    print("=" * 60)