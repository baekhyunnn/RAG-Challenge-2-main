import os
import sys
from pathlib import Path
from dotenv import load_dotenv
from langchain_chroma import Chroma
from langchain_community.embeddings import DashScopeEmbeddings

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# è·å–é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent
PDF_DIR = PROJECT_ROOT / "data" / "pdf"
VECTOR_STORE_DIR = PROJECT_ROOT / "vector_store"

# ç¡®ä¿èƒ½å¯¼å…¥åŒç›®å½•ä¸‹çš„ pdf_parsing
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from pdf_parsing import parse_pdf

def main():
    api_key = os.getenv("DASHSCOPE_API_KEY")
    if not api_key:
        print("âŒ é”™è¯¯: è¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½® DASHSCOPE_API_KEY")
        return

    # 1. åˆå§‹åŒ– Embedding æ¨¡å‹
    embeddings = DashScopeEmbeddings(model="text-embedding-v1", dashscope_api_key=api_key)

    # 2. æ‰«æå¹¶è§£æ PDF
    pdf_files = list(PDF_DIR.glob("*.pdf"))
    if not pdf_files:
        print(f"ğŸ“‚ æ–‡ä»¶å¤¹ {PDF_DIR} ä¸­æœªæ‰¾åˆ° PDF æ–‡ä»¶")
        return

    all_chunks = []
    all_metadatas = []

    for pdf_path in pdf_files:
        print(f"ğŸ“„ æ­£åœ¨è§£æ: {pdf_path.name}")
        result = parse_pdf(pdf_path)
        if result and "plain_text" in result:
            # ç®€å•åˆ†å—é€»è¾‘
            text = result["plain_text"]
            chunks = [text[i:i+1000] for i in range(0, len(text), 800)]
            all_chunks.extend(chunks)
            all_metadatas.extend([{"source": pdf_path.name} for _ in chunks])

    # 3. å†™å…¥ Chroma æ•°æ®åº“
    if all_chunks:
        print(f"ğŸ§ª æ­£åœ¨æ„å»ºå‘é‡åº“ï¼Œå…± {len(all_chunks)} ä¸ªå—...")
        Chroma.from_texts(
            texts=all_chunks,
            embedding=embeddings,
            persist_directory=str(VECTOR_STORE_DIR),
            metadatas=all_metadatas
        )
        print(f"âœ… æˆåŠŸï¼å‘é‡åº“å·²ä¿å­˜åœ¨: {VECTOR_STORE_DIR}")
    else:
        print("âš ï¸ æœªæå–åˆ°ä»»ä½•æ–‡æœ¬å†…å®¹")

if __name__ == "__main__":
    main()